{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gaussian Distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gaussian Distribution is also regarded as Normal Distribution which is represented in the form of a Bell-Shaped Curve. The most important feature of Gaussian Distribution is Mean, Median and Mode. \n",
    "\n",
    "Mean : The weighted average of the total distribution.\n",
    "Median : The Center point of the distribution when arranged in ascending order.\n",
    "Mode : The most frequent datapoints in a distribution\n",
    "\n",
    "In Normal distribution ,\n",
    "Mean = Median = Mode\n",
    "All the features are located at the center of the curve. Ideal Gaussian Distribution provides 50% of the datapoints on either side of the Mean,Median,Mode value.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Binomial Distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bi means Two. So Binomial Distribution deals with results that have two possible outcomes. It gives the measure of strictly Categorical datasets where the result will be either True/False or Yes/No or Success/Failure. \n",
    "\n",
    "A binomial experiment is a statistical experiment that has the following properties:\n",
    "\n",
    "1. The experiment consists of n repeated trials.\n",
    "2. Each trial can result in just two possible outcomes. We call one of these outcomes a success and the other, a failure.\n",
    "3. The probability of success, denoted by P, is the same on every trial.\n",
    "4. The trials are independent; that is, the outcome on one trial does not affect the outcome on other trials.\n",
    "\n",
    "The Binomial Distribution is illustrated with the help of the Probability formula which is provided as follows :\n",
    "\n",
    "b(x; n, P) = nCx * (P)x * (1 – P)n – x\n",
    "\n",
    "where,\n",
    "\n",
    "b = binomial probability\n",
    "x = total number of “successes” (pass or fail, heads or tails etc.)\n",
    "P = probability of a success on an individual trial\n",
    "n = number of trials\n",
    "\n",
    "nCx = Combination of n with respect to x\n",
    "\n",
    "n!/[x! * (n-x)!]\n",
    "\n",
    "\n",
    "The binomial distribution has the following properties:\n",
    "\n",
    "1. The mean of the distribution (μx) is equal to n * P .\n",
    "2. The variance (σ^2(x)) is n * P * ( 1 - P ).\n",
    "3. The standard deviation (σ(x)) is sqrt[ n * P * ( 1 - P ) ].\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Difference between Gaussian and Binomial Distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The main difference between Gaussian and Binomial Distribution is :\n",
    "\n",
    "Gaussian Distribution works on Continuous Dataset whereas Binomial Distribtuion wrks on Discreate dataset. \n",
    "Using Gaussian Distribution , we can find a datapoint between two given datapoint whereas we cannot find any such datapoint using Binomial Distribution.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Logistic Regression is used when the dependent variable is categorical in nature. The datapoints fit into a Linear Regression model which is then acted upon by a logistic function predicting the target variable. \n",
    "Types of Logistic Regression\n",
    "1. Binary Logistic Regression\n",
    "The categorical response has only two 2 possible outcomes. Example: Spam or Not\n",
    "2. Multinomial Logistic Regression\n",
    "Three or more categories without ordering. Example: Predicting which food is preferred more (Veg, Non-Veg, Vegan)\n",
    "3. Ordinal Logistic Regression\n",
    "Three or more categories with ordering. Example: Movie rating from 1 to 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decision Tree "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A Decision Tree is a simple representation for classifying examples. It is a Supervised Machine Learning where the data is continuously split according to a certain parameter.\n",
    "Decision Tree consists of :\n",
    "1. Nodes : Test for the value of a certain attribute.\n",
    "2. Edges/ Branch : Correspond to the outcome of a test and connect to the next node or leaf.\n",
    "3. Leaf nodes : Terminal nodes that predict the outcome (represent class labels or class distribution).\n",
    "\n",
    "\n",
    "Advantages of Classification with Decision Trees:\n",
    "1. Inexpensive to construct.\n",
    "2. Extremely fast at classifying unknown records.\n",
    "3. Easy to interpret for small-sized trees\n",
    "4. Accuracy comparable to other classification techniques for many simple data sets.\n",
    "5. Excludes unimportant features.\n",
    "\n",
    "Disadvantages of Classification with Decision Trees:\n",
    "1. Easy to overfit.\n",
    "2. Decision Boundary restricted to being parallel to attribute axes.\n",
    "3. Decision tree models are often biased toward splits on features having a large number of levels.\n",
    "4. Small changes in the training data can result in large changes to decision logic.\n",
    "5. Large trees can be difficult to interpret and the decisions they make may seem counter intuitive.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is an ensemble tree-based learning algorithm. The Random Forest Classifier is a set of decision trees from randomly selected subset of training set. It aggregates the votes from different decision trees to decide the final class of the test object.\n",
    "\n",
    "Ensemble algorithms are those which combines more than one algorithms of same or different kind for classifying objects.\n",
    "\n",
    "Features and Advantages of Random Forest :\n",
    "1. It is one of the most accurate learning algorithms available. For many data sets, it produces a highly accurate classifier.\n",
    "2. It runs efficiently on large databases.\n",
    "3. It can handle thousands of input variables without variable deletion.\n",
    "4. It gives estimates of what variables that are important in the classification.\n",
    "5. It generates an internal unbiased estimate of the generalization error as the forest building progresses.\n",
    "6. It has an effective method for estimating missing data and maintains accuracy when a large proportion of the data are missing.\n",
    "\n",
    "Disadvantages of Random Forest :\n",
    "1. Random forests have been observed to overfit for some datasets with noisy classification tasks.\n",
    "2. For data including categorical variables with different number of levels, random forests are biased in favor of those attributes with more levels. Therefore, the variable importance scores from random forest are not reliable for this type of data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
